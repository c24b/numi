{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# # Cours 5\n",
    "\n",
    "## Introductions au techniques de crawl en Python\n",
    "\n",
    "### Principes du crawl\n",
    "\n",
    "Le crawl est une technique qui consiste à naviguer sur ou plusieurs site pour en récupérer les information \n",
    "C'est la technique qu'emploient les moteurs de recherche pour créer une base de données qui contient les références\n",
    "au site et les mots clés qui correspondent à la recherche.\n",
    "La base du crawl est de stocker les liens présents dans une page, de les télécharger au fur et à mesure et de stocker les informations intéressantes.\n",
    "Le crawl permet donc de reconstituer une base de données en collectant les informations html et en les formattant pour les insérer dans une base\n",
    "\n",
    "### Les différents types de crawl\n",
    "Il existe autant de crawler que de besoins spécifiques parfois et même souvent \n",
    "une même extraction peut procéder de différentes manières et donc avoir différents algorithmes.\n",
    "\n",
    "Pour la collecte de données, on développe des crawlers spécifiques:  \n",
    "* qui suit le parcours identifié sur le site\n",
    "* a une profondeur fixée\n",
    "* se concentre uniquement sur le site en question\n",
    "* extrait les informations préalablement identifiées\n",
    "* représente souvent les données sous forme tabulaire \n",
    "\n",
    "Par opposition le crawler web que développent les moteurs de recherche\n",
    "et qu'on appelera ici *Spider*\n",
    "* suit toujours le même parcours \n",
    "* ne se concentre sur aucun site en particuler\n",
    "* n'extrait que les informations qui sont communes à tous les sites web\n",
    "* représente souvent les données sous forme de réseau ou de graphes\n",
    "\n",
    "#### Le Spider web\n",
    "##### Algorithme et implémentation\n",
    "L'algorithme d'un spider simplifié fonctionne de la manière suivante:\n",
    "\n",
    "A partir d'une url de départ:\n",
    "        * télécharger la page\n",
    "        * parser le contenu\n",
    "        * extraire toutes les urls\n",
    "        \n",
    "Pour chaque url de la liste de départ:\n",
    "    * refaire le travail\n",
    "    * et ainsi de suite jusqu'à ce qu'il n'y ait plus d'url à traiter\n",
    "\n",
    "En pseudo code python cela donnerait\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "global name 'download' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-aec6f460f861>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[0mstarter_url\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"www.example.com\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m \u001b[0mtocrawl\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcrawl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstarter_url\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m \u001b[1;32mwhile\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtocrawl\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0murl\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtocrawl\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-2-aec6f460f861>\u001b[0m in \u001b[0;36mcrawl\u001b[1;34m(url)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mtocrawl\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mcrawl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[0mhtml\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdownload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m     \u001b[0mpage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhtml\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0murls\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mextract_links\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: global name 'download' is not defined"
     ]
    }
   ],
   "source": [
    "tocrawl = []\n",
    "def crawl(url):\n",
    "    html = download(url)\n",
    "    page = parse(html)\n",
    "    urls = extract_links(page)\n",
    "    tocrawl.append(urls)\n",
    "    return tocrawl\n",
    "\n",
    "starter_url = \"www.example.com\"\n",
    "tocrawl = crawl(starter_url)\n",
    "while len(tocrawl) != 0:\n",
    "    for url in tocrawl:\n",
    "        crawl(url)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evidemment c'est un tout petit peu plus compliqué que ça...\n",
    "Quelques exemples à la fin de ce cours\n",
    "\n",
    "* Fonctionnement du Spider de Google\n",
    "* Fonctionnement de Hyphe\n",
    "* Fonctionnement de Crawtext\n",
    "* Reprendre le fonctionnement des crawlers en ligne: détailer leur algorithme\n",
    "\n",
    "##### Représentation des données\n",
    "\n",
    "Les données du spider permettent de constituer des réseaux et des graphes en fonctions des informations collectées.\n",
    "Les réseaux constitués les plus simple constistent en graphes de co-citations\n",
    "une url source etant reliée a plusieurs urls par le fait quelle les mentionnent sur la page \n",
    "on peut cartographier les sites qui se citent les uns les autres.\n",
    "\n",
    "\n",
    "####   Le crawler de site\n",
    "##### Algorithme et implémentation\n",
    "\n",
    "L'algorithme d'un crawler de site simplifié fonctionne de la manière suivante:\n",
    "Une fois le parcours sur le site et les informations identifiées, le robot aggrège les données \n",
    "cibles qu'il stocke dans une base de données ou un fichier final à plat.\n",
    "La parcours varie en fonction de l'information auquel on souhaite accéder et la manière dont on veut interroger les données.\n",
    "\n",
    "* Un exemple d'implémentation de crawl pour le site \n",
    "https://www.republique-numerique.fr/consultations/projet-de-loi-numerique/consultation/consultation\n",
    "\n",
    "L'objectif étant de récupérer l'ensemble des arguments pour chaque article de loi\n",
    "* Lister les urls de tous les participants\n",
    "    Pour chaque participants:\n",
    "    * extraire les arguments qui lui appartiennent\n",
    "\n",
    "##### Représentation des données\n",
    "Le crawl produit une base de données d'arguments reliés à un article de loi\n",
    "Dans le cas d'un crawl sur un site particulier, il est capital de définir avant le crawl \n",
    "les données à extraire et les différentes étapes pour appliquer l'extraction à plusieurs pages\n",
    "\n",
    "### TP: construction d'un crawler de site web\n",
    "\n",
    "#### Extracteur d'une page web\n",
    "\n",
    "Reprenons toutes les étapes d'extraction pour la page d'un site.\n",
    "On va reprendre les bases à travers plusieurs exercices\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Exercice 0: Extraire les liens d'une page web\n",
    "\n",
    "Vous pouvez télécharger le [script complet d'exemple par ici](./script0.py)\n",
    "ou copier coller dans votre éditeur le code à la suite\n",
    "Il s'agit d'un script très simple dont il faut bien lire les instructions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La liste de toutes les urls de cette page de départ est stockées dans liens1\n",
    "pour savoir combien d'url on a récupéré on peut appeler une fonction standard dans python\n",
    "len(uneliste) qui a partir d'une liste en parametres donne le nombre d'élements présents dans la liste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/python \n",
    "# coding:utf-8\n",
    "\n",
    "#ici les modules\n",
    "import requests\n",
    "import csv\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "BeautifulSoup(\"html.parser\")\n",
    "\n",
    "#ici les fonctions\n",
    "def download(url):\n",
    "    '''fonction qui télécharge une page a partir d'une url et retourne le html sous forme de texte'''\n",
    "    reponse = requests.get(url)\n",
    "    if reponse.status_code == 200:\n",
    "        return reponse.text\n",
    "\n",
    "def write_csv(filename, line):\n",
    "    '''fonction qui écrit des données sous forme de listes dans un fichier []'''\n",
    "    with open(filename, 'a') as csvfile: \n",
    "        spamwriter = csv.writer(csvfile, delimiter=';')\n",
    "        spamwriter.writerow(line)\n",
    "    return\n",
    "\n",
    "def extract_links(html):\n",
    "    '''fonction qui extrait simplement les liens de la page'''\n",
    "    \n",
    "    liens = []\n",
    "    soup = BeautifulSoup(html)\n",
    "    tag_link_list = soup.find_all(\"a\")\n",
    "    for element in tag_link_list:\n",
    "        lien = element.get(\"href\")\n",
    "        liens.append(lien)\n",
    "    return liens\n",
    "\n",
    "###################################################################\"\n",
    "### c'est ici que l'on execute notre programme\n",
    "### en appelant les fonctions qu'on a définit plus haut\n",
    "#**********************************#\n",
    "#Exemple 0:\n",
    "#Extraire simplement les urls de la page\n",
    "#**********************************#\n",
    "print \"Exemple0: Extraire simplement les urls de la page\"\n",
    "\n",
    "url_de_depart='http://www.bbc.com/news/science_and_environment'\n",
    "html = download(url_de_depart)\n",
    "liens0 = extract_links(html)\n",
    "print \"Il y a %i urls sur la page %s\" %(len(liens0), url_de_depart)\n",
    "write_csv(\"test0.csv\",liens0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Exercice 1: Choississez une page\n",
    "reprenez le script0.py et enregistrez le sous script1.py\n",
    "\n",
    "Changez maintenant la partie ou l'on fat tourner le code\n",
    "en changeant l'url_de_depart par l'url de votre choix\n",
    "en suivant l'exemple ci dessous on en téléchargeant le [code par ici](./script1.py)\n",
    "et en le modifiant un peu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "###################################################################\"\n",
    "### c'est ici que l'on execute notre programme\n",
    "### en appelant les fonctions qu'on a définit plus haut\n",
    "#**********************************#\n",
    "#Exemple 1:\n",
    "#Extraire simplement les urls de la page de mon choix\n",
    "#**********************************#\n",
    "print \"Exemple1: Extraire simplement les urls de la page de mon choix\"\n",
    "\n",
    "url= \"http://www.opentechschool.org/\"\n",
    "html = download(url)\n",
    "liens0 = extract_links(html)\n",
    "print \"Il y a %i urls sur la page %s\" %(len(liens0), url)\n",
    "write_csv(\"test1.csv\",liens0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Bon jusque là c'était facile, on vérifie juste que vous suivez\n",
    "On va maintenant montrer quelques methodes pour extraire des contenus\n",
    "et les méthodes spécifiques à BeautifulSoup\n",
    "A vous de tester!\n",
    "\n",
    "###### Exercice 2: Extraire les données d'une seule page de chanson\n",
    "Normalement vous avez déjà réussi à faire ce travail la semaine dernière ,\n",
    "il s'agit simplement ici de completer la fonction extract_chanson\n",
    "en ajoutant les méthodes pour extraire le titre, l'auteur, le texte et les tags d'une page de chanson\n",
    "a partir de BeautifulSoup.\n",
    "\n",
    "Pour avoir le détail du fonctionnement de BeautifulSoup, \n",
    "je vous donne deux exemples détaillés\n",
    "des methodes find et find_all et de co\n",
    "mment récupérer des données internes à la balise \n",
    "(par exemple href ou scr):\n",
    "* [exemple1: script1.py] (../script1.py)\n",
    "* [exemple2: script2.py] (../script2.py)\n",
    "\n",
    "Maintenant que vous savez et avez réussi à extraire ce dont nous avions besoin\n",
    "vous pouvez ajouter votre extraction dans la fonction extract_chanson()/\n",
    "Le code complet à completer est téléchargeable aussi par [ici](../script3.py)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/python \n",
    "# coding:utf-8\n",
    "\n",
    "#ici les modules\n",
    "import requests\n",
    "import csv\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "BeautifulSoup(\"html.parser\")\n",
    "\n",
    "#ici les fonctions\n",
    "def download(url):\n",
    "    '''fonction qui télécharge une page a partir d'une url et retourne le html sous forme de texte'''\n",
    "    reponse = requests.get(url)\n",
    "    if reponse.status_code == 200:\n",
    "        return reponse.text\n",
    "\n",
    "def write_csv(filename, line):\n",
    "    '''fonction qui écrit des données sous forme de listes dans un fichier []'''\n",
    "    with open(filename, 'a') as csvfile: \n",
    "        spamwriter = csv.writer(csvfile, delimiter=';')\n",
    "        spamwriter.writerow(line)\n",
    "    return\n",
    "\n",
    "def extract_links(url):\n",
    "    '''fonction qui extrait simplement les liens de la page'''\n",
    "    print url\n",
    "    html = download(url)\n",
    "    liens = []\n",
    "    soup = BeautifulSoup(html)\n",
    "    tag_link_list = soup.find_all(\"a\")\n",
    "    for element in tag_link_list:\n",
    "        lien = element.get(\"href\")\n",
    "        liens.append(lien)\n",
    "    return liens\n",
    "\n",
    "def extract_images(html):\n",
    "    '''extrait les videos depuis la page html de BBC NEWS SCIENCE'''\n",
    "    soup = BeautifulSoup(html)\n",
    "    colonne_videos = soup.find(\"div\", {\"id\": \"comp-candy-asset-munger\"})    \n",
    "    img_tags = colonne_videos.find_all(\"img\")\n",
    "    images = []\n",
    "    for img in img_tags:\n",
    "        image_sources = img.get(\"scr\")\n",
    "        images.append(image_sources)\n",
    "    return images\n",
    "    \n",
    "def extract_titles(html):\n",
    "    '''extrait les videos depuis la page html de BBC NEWS SCIENCE'''\n",
    "    soup = BeautifulSoup(html)\n",
    "    videos_titres = []\n",
    "    colonne_videos = soup.find(\"div\", {\"id\": \"comp-candy-asset-munger\"})    \n",
    "    videos_items = colonne_videos.findall(\"div\",{\"class\":\"condor-item\"})\n",
    "    for v in videos_items:\n",
    "        titre = v.h3.span.getText()\n",
    "        videos_titres.append(titre)\n",
    "    return data\n",
    "\n",
    "def extract_chanson(html):\n",
    "    '''ici la fonction qui permet d'extraire \n",
    "    le texte de la chanson \n",
    "    le titre\n",
    "    les tags \n",
    "    l'auteur\n",
    "    en utilisant les fonctions de Beautiful Soup\n",
    "    '''\n",
    "    soup = BeautifulSoup(html)\n",
    "    \n",
    "    texte = \"\"\n",
    "    #Indice pour nettoyer le texte qui compporte sans doute des sauts de lignes et des caractères pas sympas\n",
    "    #on utilise cette function standard qui remplace les caractères non desiré par un espace\n",
    "    texte = texte.sub(\"\\n|\\t|\\s\\s\", \" \")\n",
    "    titre = \"\"\n",
    "    tags = \"\"\n",
    "    auteur = \"\"\n",
    "    #Attention les tags sont une liste et comme nous allons  ecrire dans un fichier csv\n",
    "    #il faut applatir les données des tags qui sont une liste\n",
    "    #avec la methode join qui transforme une liste d'element en une chaine de caractère\n",
    "    #nous allons delimiter les tags entre eux par ***\n",
    "    tags = tags.join(\"***\")\n",
    "    return [titre, texte, tags, auteur]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si vous le testez là tel quel et juste en ayant ajouter les information: il ne se passera rien \n",
    "Nous avons juste importé des modules et défini des fonctions il faut maintenant les appeler comme suit dans l'exemple\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print \"Exemple3: La chanson de mon choix:\"\n",
    "html = download(\"http://genius.com/Ab-soul-terrorist-threats-lyrics\")\n",
    "line = extract_chanson(html)\n",
    "print line\n",
    "write_csv(\"test3.csv\", line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Crawler de site: implémentation\n",
    "Maintenant que vous avez un extracteur pour une chanson il est très facile de coder un extracteur pour toutes les chansons.\n",
    "Il faut avant tout définir le parcours du crawl.\n",
    "\n",
    "Vous allez fermer vos ordinateurs pour un moment et je vous montrer l'exemple"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vous allez détailler les étapes à la main en suivant mon exemple.\n",
    "\n",
    "Je cherche à extraire tous les arguments du site https://www.republique-numerique.fr/consultations/projet-de-loi-numerique/consultation/consultation pour chaque article\n",
    "\n",
    "Etant donné qu'il est très difficile de récupérer les articles et les arguments\n",
    "j'ai choisi d'attaquer par le profil des participants:\n",
    "\n",
    "1. J'ai défini une fonction qui a partir d'une page de profil\n",
    "\n",
    "qui extrait les informations suivantes\n",
    "[auteur, date, argument, lien_article]\n",
    "\n",
    "2. J'ai defini une fonction qui a partir de la page des participants\n",
    "qui extrait les url d'un profil\n",
    "\n",
    "3. J'ai defini une fonction qui crée la liste de chaque page où sont listé les participants\n",
    "qui génére une liste de page à extraire\n",
    "\n",
    "4. J'ai maintenant les élements pour présenter mon algorithme et mon choix d'implémentation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "liste_pages_profiles = lister_pages_participants()\n",
    "for page in liste_pages_profiles:\n",
    "    liste_profil = extract_url_profile(page)\n",
    "    for profile_url in liste_profil:\n",
    "        for arguments in extract_profile(profile_url):\n",
    "            csv_writer(\"arguments.csv\", arguments)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Maintenant le détail de l'implémentation sans les véritables valeurs de BeautifulSoup (pour aller plus vite)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def extract_profile(url):\n",
    "    infos = []\n",
    "    html = download(url)\n",
    "    soup = BeautifulSoup(html)\n",
    "    auteur = soup.find(\"div\", {\"class\":auteur})\n",
    "    auteur_url = url\n",
    "    arguments = soup.findall(\"div\", {\"class\": \"arguments\"})\n",
    "    for argument in arguments:\n",
    "        date = argument.find(\"div\", {\"class\":\"date\"})\n",
    "        article = argument.find(\"a\")\n",
    "        lien_article = article.get(\"href\")\n",
    "        text = argument.find(\"p\").text\n",
    "        infos.append([auteur, date, texte, lien_article])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Je suis donc déjà capable d'extraire une liste d'argument pour un utilisateur\n",
    "en prenant exemple sur le profile suivant: https://www.republique-numerique.fr/profile/jeannevarasco\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "url = \"https://www.republique-numerique.fr/profile/jeannevarasco\"\n",
    "arguments = extract_profile(url)\n",
    "for a in arguments:\n",
    "    write_csv(\"arguments.csv\", a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Maintenant comment faire pour récupérer tous les arguments?\n",
    "\n",
    "\n",
    "Tous les participants sont listés à une meme adresse de la page 1 à la page 1334\n",
    "\n",
    "\n",
    "Je vais donc définir une fonction qui extrait l'url du profile du participants depuis une page d'exemple\n",
    "https://www.republique-numerique.fr/projects/projet-de-loi-numerique/participants/2\n",
    "\n",
    "et qui me donne les urls de profile présents sur cette page\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def extract_participants_url(url):\n",
    "    participants = []\n",
    "    html = download(url)\n",
    "    soup = BeautifulSoup(html)\n",
    "    for p in soup.find_all(\"div\"):\n",
    "        url = \"\"\n",
    "        participants.append(url)\n",
    "    return participants"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "J'ai récupéré maintenant pour une page d'exemple les 16 urls de profiles\n",
    "je peux donc pour chaque page extraire chaque profile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "url = \"https://www.republique-numerique.fr/projects/projet-de-loi-numerique/participants/2\"\n",
    "list_participants = extract_participants_url(url)\n",
    "for profile in list_participants:\n",
    "    arguments = extract_profile(profile)\n",
    "    for a in arguments:\n",
    "        write_csv(\"arguments.csv\", a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il me faut donc maintenant tourner virtuellement les pages du site\n",
    "rien de plus simple les pages sont numérotées!\n",
    "je vais donc générer une liste de page\n",
    "structurées comme suit:\n",
    "`url = \"https://www.republique-numerique.fr/projects/projet-de-loi-numerique/participants/\"+\"1\"`\n",
    "\n",
    "etc... \n",
    "\n",
    "jusqu'à 1334\n",
    "dans une petite fonction qui s'appelle liste_pages_profiles()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def liste_pages_profiles():\n",
    "    pages = []\n",
    "    url = \"https://www.republique-numerique.fr/projects/projet-de-loi-numerique/participants/\"\n",
    "    for page_nb in range(1, 1335):\n",
    "        page_url = url+str(page_nb)\n",
    "        pages.append(url)\n",
    "    return pages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "A vous de jouer avec genius....\n",
    "\n",
    "Lister le parcours et les extractions à chaque étape\n",
    "\n",
    "Puis on le codera ensemble...\n",
    "\n",
    "C'est parti!\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
