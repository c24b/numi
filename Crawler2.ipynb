{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# # Cours 5\n",
    "\n",
    "## Introductions au techniques de crawl en Python\n",
    "\n",
    "### Principes du crawl\n",
    "\n",
    "Le crawl est une technique qui consiste à naviguer sur ou plusieurs site pour en récupérer les information \n",
    "C'est la technique qu'emploient les moteurs de recherche pour créer une base de données qui contient les références\n",
    "au site et les mots clés qui correspondent à la recherche.\n",
    "La base du crawl est de stocker les liens présents dans une page, de les télécharger au fur et à mesure et de stocker les informations intéressantes.\n",
    "Le crawl permet donc de reconstituer une base de données en collectant les informations html et en les formattant pour les insérer dans une base\n",
    "\n",
    "\n",
    "Un crawler simple peut se coder simplement en python en écrivant un programme composé de ces simples instructions:\n",
    "\n",
    "A partir d'une url de départ:\n",
    "        * télécharger la page\n",
    "        * extraire toutes les urls\n",
    "        * les stocker dans une liste\n",
    "        * extraire les autres informations utiles \n",
    "        * les insérer dans un fichier\n",
    "\n",
    "Pour chaque url de la liste de départ:\n",
    "* refaire le travail\n",
    "et ainsi de suite jusqu'à ce qu'il n'y ait plus d'url à traiter\n",
    "        \n",
    "### Rappel des différentes fonctions\n",
    "\n",
    "Construisons ensemble notre premier crawler appelé crawl.py\n",
    "Definissons d'abord en haut du fichier en quelle langage de programation nous ecrivons:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/python "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Définissons ensuite l'encodage pour prendre en compte les accents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# coding: utf-8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On importe au debut du script \n",
    "tous les modules complémentaire \n",
    "dont on va avoir besoin et qui sont importer grace à l'instruction import\n",
    "\n",
    "* un module de téléchargement \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* un module qui permet de parcourir le html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour BeautifulSoup, on spécifie le type parser que nous allons utiliser\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "BeautifulSoup(\"html.parser\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "On va ensuite écrire les fonctions dont on a besoin:\n",
    "    \n",
    "    * une fonction pour télécharger les urls ``download``\n",
    "    \n",
    "    * une fonction pour écrire dans un fichier ``write_csv``\n",
    "    \n",
    "    * une fonction pour extraire les liens ``extract_links``\n",
    "    \n",
    "    * une fonction pour extraire les données intéressantes ``extract_data``\n",
    "\n",
    "Pour les utiliser ensuite (les appeler) dans notre fichier à la fin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "def download(url):\n",
    "    '''fonction qui télécharge une page a partir d'une url et retourne le html sous forme de texte'''\n",
    "    #on appelle le module requests qui permet de télécharger la page\n",
    "    #et on stocke le résultat du téléchargement\n",
    "    #dans une variable qu'on appelle ici reponse\n",
    "    reponse = requests.get(url)\n",
    "    #response est produite par le module requests\n",
    "    #elle a donc des valeurs spécifiques qui sont documentés sur la page du module\n",
    "    # requests \n",
    "    #tel que le code de status de la page cf[[Les codes d'erreur HTTP\n",
    "    # le texte ou encore un json\n",
    "    if response.status_code == 200:\n",
    "        return r.text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L'avantage d'écrire une **fonction** est qu'elle peut etre utilisée autant de fois  \n",
    "qu'on veut avec n'importe quelle **variable** c'est à dire pour cette fonction download\n",
    "avec n'importe quelle url\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#ici la fonction pour écrire une ligne dans un fichier CSV\n",
    "def write_csv(filename, data):\n",
    "    #on ouvre le fichier appelé filename comme un fichier csv\n",
    "    # a pour ajouter à la suite (append)\n",
    "    #r pour lire (read)\n",
    "    #w pour ecrire (write)\n",
    "    with open(filename, 'a') as csvfile: \n",
    "        #on met le fichier dans le writer de csv\n",
    "        #en specifiant la délimitation ici \";\"\n",
    "        spamwriter = csv.writer(csvfile, delimiter=';')\n",
    "        #data doit etre une liste d'éléments représentée en python par []\n",
    "        spamwriter.writerow(data)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#ici la fonction pour extraire des liens a partir d'une url\n",
    "def extract_links(url):\n",
    "    #on télécharge le contenu de la page\n",
    "    #avec une fonction qu'on a déjà écrite plus haut\n",
    "    html = download(page)\n",
    "    #on va stocker tous les liens contenu dans la page dans une liste\n",
    "    #avec le nom\n",
    "    links = []\n",
    "    soup = BeautifulSoup(html)\n",
    "    \n",
    "    #la fonction find_all renvoie une **liste**\n",
    "    #de plusieurs elements BeautifulSoup\n",
    "    # qui sont manipulable en utilisant les fonctions de Beautifulsoup\n",
    "    #on a besoin ici de récupérer le lein contenu de la balise a href=\"{{lein}}\"\n",
    "    #qu'on va stocker dans notre liste comme dans un tableau\n",
    "    \n",
    "    #ici on veut récupérer toutes les balises de type a sans filtre particulier\n",
    "    tag_link_list = soup.find_all(\"a\")\n",
    "    #name est donc une liste d'element parsés de tags\n",
    "    #on va donc derouler les élements contenu\n",
    "    for element in tag_link_list:\n",
    "        #l'url de la page est stocké\n",
    "        #dans le corps de la balise\n",
    "        #exemple <a href=\"http://lemonde.fr\">Site du Monde </a>\n",
    "        #pour le récupérer on utilise la fonction de Beautifulsoup get\n",
    "        lien = element.get(\"href\")\n",
    "        #on ajoute le lien à la liste\n",
    "        liens.append(lien)\n",
    "    return liens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#ici la fonction qui permet d'extraire les informations interessantes\n",
    "#pour un site particulier\n",
    "def extract_data(url):\n",
    "    #on télécharge le contenu de la page\n",
    "    #avec une fonction qu'on a déjà écrite plus haut\n",
    "    html = download(page)\n",
    "    #on transforme le html en le parcourant (parsing) avec BeautifulSoup\n",
    "    soup = BeautifulSoup(html)\n",
    "    #on va stocker tous les futures informations \n",
    "    #dans une liste qu'on appelle data\n",
    "    data = []\n",
    "    #pour extraire les données on utilise les fonctions de BeautifulSoup\n",
    "    #find_all renvoie une liste d'elements\n",
    "    #find renvoie le premier élement\n",
    "    \n",
    "    #imaginons que nous voulons extraire toutes les videos de cette page\n",
    "    #http://www.bbc.com/news/science_and_environment\n",
    "    #elles sont contenues dans la balise suivante\n",
    "    #<div id=\"comp-candy-asset-munger\" class=\"distinct-component-group container-condor wide-only\">\n",
    "    #on peut découper la partie qui nous interesse\n",
    "    #en utilisant find qui renvoie la première balise\n",
    "    # et l'id qui permet de trouver cette balise particuliere\n",
    "    colonne_videos = soup.find(\"div\", {\"id\": \"comp-candy-asset-munger\"})\n",
    "    #on peut ensuite appliquer les méthodes de BeautifulSoup à l'interieur de cette partie\n",
    "    #recuperer toutes les images en utilisant find_all\n",
    "    img_tags = colonne_videos.findall(\"img\", {\"class\":\"responsive-image__inner-for-label\"})\n",
    "    #il s'agit encore d'une liste de tag\n",
    "    \n",
    "    #on va récupérer la source de cette image stockée dans src\n",
    "    # et les stocker dans une liste spéciale pour ces données\n",
    "    images = []\n",
    "    for img in img_tags:\n",
    "        image_sources = img.get(\"scr\")\n",
    "        images.append(image_sources)\n",
    "    #on peut aussi se promener dans l'arborescence du HTML qu'on voit dans l'explorateur\n",
    "    #par exemple je veux récupérer tous les titres des videos contenu dans les items >h3>span\n",
    "    #je crée une nouvelle liste\n",
    "    videos_titres = []\n",
    "    videos_items = colonne_videos.findall(\"div\",{\"class\":\"condor-item\"})\n",
    "    for v in videos_items:\n",
    "        #ici je veux stocker le texte et non plus le tag BeautifulSoup\n",
    "        #j'appelle donc la fonction getText() fournie par BeautifulSoup\n",
    "        titre = v.h3.span.getText()\n",
    "        videos_titres.append(titre)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Toutes ces fonctions toutes sont universelles et s'applique à n'importe quelle page html\n",
    "sauf la derniere `extract_data()` qui extrait les données a partir de tag qui n'existent que pour ce site et cette page précise.\n",
    "Vous pouvez télécharger la version propre et complete depuis ce fichier \n",
    "\n",
    "Maintenant nous allons appeler les différentes fonctions pour faire marcher notre crawler\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#### Exemple d'un crawl simple à profondeur 1\n",
    "Ici le code de base pour extraire tous les liens d'une page de départ:\n",
    "on part d'une url de départ qu'on télécharge et dont on extrait les liens qu'on stocke dans une liste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "url_de_depart='http://www.bbc.com/news/science_and_environment'\n",
    "liens_page0 = extract_links(url_de_depart)\n",
    "liens1 = []\n",
    "# on déroule les urls de la page\n",
    "for page in liens_page0:\n",
    "    # pour chaque lien \n",
    "    for lien in extract_links(page):\n",
    "        #on ajoute dans la liste\n",
    "        liens.append(lien)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La liste de toutes les urls de cette page de départ est stockées dans liens1\n",
    "pour savoir combien d'url on a récupéré on peut appeler une fonction standard dans python\n",
    "len(uneliste) qui a partir d'une liste en parametres donne le nombre d'élements présents dans la liste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nb_liens1 = len(liens1)\n",
    "print nb_liens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Crawl simple d'urls\n",
    "Maintenant imaginons que nous soyons un simple crawler qui collecte toutes les url \n",
    "sans rien stocker de plus que des liens mais à profondeur infinie\n",
    "\n",
    "On a besoin d'une liste d'url à traiter et d'une liste d'url déjà traitée pour s'assurer \n",
    "que le robot ne tourne pas à l'infini et qu'il ne crawl pas plusieurs fois une même url\n",
    "pour la même raison il est utile aussi de vérifier qu'une url n'est pas en double dans les deux listes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "url_de_depart='http://www.bbc.com'\n",
    "#liste d'url a traiter\n",
    "to_do = []\n",
    "#liste d'url traitées\n",
    "done = []\n",
    "\n",
    "starter = extract_links(url_de_depart)\n",
    "# on déroule les urls de la page\n",
    "for page in starter:\n",
    "    # pour chaque lien \n",
    "    for lien in extract_links(page):\n",
    "        #on l'ajoute à faire\n",
    "        to_do.append(lien)\n",
    "    #on ajoute la page dont les liens ont déjà ete extrait\n",
    "    done.append(page)\n",
    "\n",
    "#while est une instruction spéciale en python de la manière que for c'est une boucle\n",
    "#while permet d'executer des instructions tant que la condition est remplie\n",
    "#ici tant que la liste de to_do n'est pas vide\n",
    "while len(to_do) != 0:\n",
    "    #on enlève les éventuels doublons avec la fonction de python set() qui enlève les éléments en double\n",
    "    uniq_to_do = set(to_do)\n",
    "    for lien in uniq_to_do:\n",
    "        for new_link in extract_links(lien):\n",
    "            if new_link not in done:\n",
    "                to_do.append(new_link)\n",
    "        done.append(lien)\n",
    "        to_do.remove(lien)\n",
    "        \n",
    "#ici on aura donc au final une liste de url crawlées stockées dans la liste done\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
