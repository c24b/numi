{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# # Cours 5\n",
    "\n",
    "## Introductions au techniques de crawl en Python\n",
    "\n",
    "### Principes du crawl\n",
    "\n",
    "Le crawl est une technique qui consiste à naviguer sur ou plusieurs site pour en récupérer les information \n",
    "C'est la technique qu'emploient les moteurs de recherche pour créer une base de données qui contient les références\n",
    "au site et les mots clés qui correspondent à la recherche.\n",
    "La base du crawl est de stocker les liens présents dans une page, de les télécharger au fur et à mesure et de stocker les informations intéressantes.\n",
    "Le crawl permet donc de reconstituer une base de données en collectant les informations html et en les formattant pour les insérer dans une base\n",
    "\n",
    "### Les différents types de crawl\n",
    "Il existe autant de crawler que de besoins spécifiques parfois et même souvent \n",
    "une même extraction peut procéder de différentes manières et donc avoir différents algorithmes.\n",
    "\n",
    "Pour la collecte de données, on développe des crawlers spécifiques:  \n",
    "* qui suit le parcours identifié sur le site\n",
    "* a une profondeur fixée\n",
    "* se concentre uniquement sur le site en question\n",
    "* extrait les informations préalablement identifiées\n",
    "* représente souvent les données sous forme tabulaire \n",
    "\n",
    "Par opposition le crawler web que développent les moteurs de recherche\n",
    "et qu'on appelera ici *Spider*\n",
    "* suit toujours le même parcours \n",
    "* ne se concentre sur aucun site en particuler\n",
    "* n'extrait que les informations qui sont communes à tous les sites web\n",
    "* représente souvent les données sous forme de réseau ou de graphes\n",
    "\n",
    "#### Le Spider web\n",
    "##### Algorithme et implémentation\n",
    "L'algorithme d'un spider simplifié fonctionne de la manière suivante:\n",
    "\n",
    "A partir d'une url de départ:\n",
    "        * télécharger la page\n",
    "        * parser le contenu\n",
    "        * extraire toutes les urls\n",
    "        \n",
    "Pour chaque url de la liste de départ:\n",
    "    * refaire le travail\n",
    "    * et ainsi de suite jusqu'à ce qu'il n'y ait plus d'url à traiter\n",
    "\n",
    "En pseudo code python cela donnerait\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tocrawl = []\n",
    "def crawl(url):\n",
    "    html = download(url)\n",
    "    page = parse(html)\n",
    "    urls = extract_links(page)\n",
    "    tocrawl.append(urls)\n",
    "    return tocrawl\n",
    "\n",
    "starter_url = \"www.example.com\"\n",
    "tocrawl = crawl(starter_url)\n",
    "while len(tocrawl) != 0:\n",
    "    for url in tocrawl:\n",
    "        crawl(url)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evidemment c'est un tout petit peu plus compliqué que ça...\n",
    "Quelques exemples...\n",
    "\n",
    "Fonctionnement du Spider de Google\n",
    "Fonctionnement de Hyphe\n",
    "Fonctionnement de Crawtext\n",
    "Reprendre le fonctionnement des crawlers en ligne: détailer leur algorithme\n",
    "\n",
    "##### Représentation des données\n",
    "\n",
    "Les données du spider permettent de constituer des réseaux et des graphes en fonctions des informations collectées.\n",
    "Les réseaux constitués les plus simple constistent en graphes de co-citations\n",
    "une url source etant reliée a plusieurs urls par le fait quelle les mentionnent sur la page \n",
    "on peut cartographier les sites qui se citent les uns les autres.\n",
    "\n",
    "\n",
    "####   Le crawler de site\n",
    "##### Algorithme et implémentation\n",
    "\n",
    "L'algorithme d'un crawler de site simplifié fonctionne de la manière suivante:\n",
    "Une fois le parcours sur le site et les informations identifiées, le robot aggrège les données \n",
    "cibles qu'il stocke dans une base de données ou un fichier final à plat.\n",
    "La parcours varie en fonction de l'information auquel on souhaite accéder et la manière dont on veut interroger les données.\n",
    "\n",
    "* Un exemple d'implémentation de crawl pour le site \n",
    "https://www.republique-numerique.fr/consultations/projet-de-loi-numerique/consultation/consultation\n",
    "\n",
    "L'objectif étant de récupérer l'ensemble des arguments pour chaque article de loi\n",
    "* Lister les urls de tous les participants\n",
    "    Pour chaque participants:\n",
    "    * extraire les arguments qui lui appartiennent\n",
    "\n",
    "##### Représentation des données\n",
    "Le crawl produit une base de données d'arguments reliés à un article de loi\n",
    "Dans le cas d'un crawl sur un site particulier, il est capital de définir avant le crawl \n",
    "les données à extraire et les différentes étapes pour appliquer l'extraction à plusieurs pages\n",
    "\n",
    "### TP: construction d'un crawler de site web\n",
    "\n",
    "#### Extracteur d'une page web\n",
    "Reprenons les étapes d'extraction pour la page d'un site.\n",
    "à travers plusieurs exercices\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Exercice 0: Extraire les liens d'une page web\n",
    "Vous pouvez télécharger le [script complet d'exemple par ici] (./script0.py)\n",
    "ou le copier coller dans votre éditeur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/python "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Définissons ensuite l'encodage pour prendre en compte les accents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# coding: utf-8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On importe au debut du script \n",
    "tous les modules complémentaire \n",
    "dont on va avoir besoin et qui sont importer grace à l'instruction import\n",
    "\n",
    "* un module de téléchargement \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* un module qui permet de parcourir le html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour BeautifulSoup, on spécifie le type parser que nous allons utiliser\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "BeautifulSoup(\"html.parser\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "On va ensuite écrire les fonctions dont on a besoin:\n",
    "    \n",
    "    * une fonction pour télécharger les urls ``download``\n",
    "    \n",
    "    * une fonction pour écrire dans un fichier ``write_csv``\n",
    "    \n",
    "    * une fonction pour extraire les liens ``extract_links``\n",
    "    \n",
    "    * une fonction pour extraire les données intéressantes ``extract_data``\n",
    "\n",
    "Pour les utiliser ensuite (les appeler) dans notre fichier à la fin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "def download(url):\n",
    "    '''fonction qui télécharge une page a partir d'une url et retourne le html sous forme de texte'''\n",
    "    #on appelle le module requests qui permet de télécharger la page\n",
    "    #et on stocke le résultat du téléchargement\n",
    "    #dans une variable qu'on appelle ici reponse\n",
    "    reponse = requests.get(url)\n",
    "    #response est produite par le module requests\n",
    "    #elle a donc des valeurs spécifiques qui sont documentés sur la page du module\n",
    "    # requests \n",
    "    #tel que le code de status de la page cf[[Les codes d'erreur HTTP\n",
    "    # le texte ou encore un json\n",
    "    if response.status_code == 200:\n",
    "        return r.text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L'avantage d'écrire une **fonction** est qu'elle peut etre utilisée autant de fois  \n",
    "qu'on veut avec n'importe quelle **variable** c'est à dire pour cette fonction download\n",
    "avec n'importe quelle url\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#ici la fonction pour écrire une ligne dans un fichier CSV\n",
    "def write_csv(filename, data):\n",
    "    #on ouvre le fichier appelé filename comme un fichier csv\n",
    "    # a pour ajouter à la suite (append)\n",
    "    #r pour lire (read)\n",
    "    #w pour ecrire (write)\n",
    "    with open(filename, 'a') as csvfile: \n",
    "        #on met le fichier dans le writer de csv\n",
    "        #en specifiant la délimitation ici \";\"\n",
    "        spamwriter = csv.writer(csvfile, delimiter=';')\n",
    "        #data doit etre une liste d'éléments représentée en python par []\n",
    "        spamwriter.writerow(data)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#ici la fonction pour extraire des liens a partir d'une url\n",
    "def extract_links(url):\n",
    "    #on télécharge le contenu de la page\n",
    "    #avec une fonction qu'on a déjà écrite plus haut\n",
    "    html = download(page)\n",
    "    #on va stocker tous les liens contenu dans la page dans une liste\n",
    "    #avec le nom\n",
    "    links = []\n",
    "    soup = BeautifulSoup(html)\n",
    "    \n",
    "    #la fonction find_all renvoie une **liste**\n",
    "    #de plusieurs elements BeautifulSoup\n",
    "    # qui sont manipulable en utilisant les fonctions de Beautifulsoup\n",
    "    #on a besoin ici de récupérer le lein contenu de la balise a href=\"{{lein}}\"\n",
    "    #qu'on va stocker dans notre liste comme dans un tableau\n",
    "    \n",
    "    #ici on veut récupérer toutes les balises de type a sans filtre particulier\n",
    "    tag_link_list = soup.find_all(\"a\")\n",
    "    #name est donc une liste d'element parsés de tags\n",
    "    #on va donc derouler les élements contenu\n",
    "    for element in tag_link_list:\n",
    "        #l'url de la page est stocké\n",
    "        #dans le corps de la balise\n",
    "        #exemple <a href=\"http://lemonde.fr\">Site du Monde </a>\n",
    "        #pour le récupérer on utilise la fonction de Beautifulsoup get\n",
    "        lien = element.get(\"href\")\n",
    "        #on ajoute le lien à la liste\n",
    "        liens.append(lien)\n",
    "    return liens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#ici la fonction qui permet d'extraire les informations interessantes\n",
    "#pour un site particulier\n",
    "def extract_data(url):\n",
    "    #on télécharge le contenu de la page\n",
    "    #avec une fonction qu'on a déjà écrite plus haut\n",
    "    html = download(page)\n",
    "    #on transforme le html en le parcourant (parsing) avec BeautifulSoup\n",
    "    soup = BeautifulSoup(html)\n",
    "    #on va stocker tous les futures informations \n",
    "    #dans une liste qu'on appelle data\n",
    "    data = []\n",
    "    #pour extraire les données on utilise les fonctions de BeautifulSoup\n",
    "    #find_all renvoie une liste d'elements\n",
    "    #find renvoie le premier élement\n",
    "    \n",
    "    #imaginons que nous voulons extraire toutes les videos de cette page\n",
    "    #http://www.bbc.com/news/science_and_environment\n",
    "    #elles sont contenues dans la balise suivante\n",
    "    #<div id=\"comp-candy-asset-munger\" class=\"distinct-component-group container-condor wide-only\">\n",
    "    #on peut découper la partie qui nous interesse\n",
    "    #en utilisant find qui renvoie la première balise\n",
    "    # et l'id qui permet de trouver cette balise particuliere\n",
    "    colonne_videos = soup.find(\"div\", {\"id\": \"comp-candy-asset-munger\"})\n",
    "    #on peut ensuite appliquer les méthodes de BeautifulSoup à l'interieur de cette partie\n",
    "    #recuperer toutes les images en utilisant find_all\n",
    "    img_tags = colonne_videos.find_all(\"img\", {\"class\":\"responsive-image__inner-for-label\"})\n",
    "    #il s'agit encore d'une liste de tag\n",
    "    #on va récupérer la source de cette image stockée dans src\n",
    "    for img in img_tags:\n",
    "        image_sources = img.get(\"scr\")\n",
    "        \n",
    "        data.append([image_sources, )\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Toutes ces fonctions toutes sont universelles et s'applique à n'importe quelle page html\n",
    "sauf la derniere `extract_data()` qui extrait les données a partir de tag qui n'existent que pour ce site et cette page précise.\n",
    "Vous pouvez télécharger la version propre et complete depuis ce fichier \n",
    "\n",
    "Maintenant nous allons appeler les différentes fonctions pour faire marcher notre crawler\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#### Exemple d'un crawl simple à profondeur 1\n",
    "Ici le code de base pour extraire tous les liens d'une page de départ:\n",
    "on part d'une url de départ qu'on télécharge et dont on extrait les liens qu'on stocke dans une liste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "url_de_depart='http://www.bbc.com/news/science_and_environment'\n",
    "liens_page0 = extract_links(url_de_depart)\n",
    "liens1 = []\n",
    "# on déroule les urls de la page\n",
    "for page in liens_page0:\n",
    "    # pour chaque lien \n",
    "    for lien in extract_links(page):\n",
    "        #on ajoute dans la liste\n",
    "        liens.append(lien)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La liste de toutes les urls de cette page de départ est stockées dans liens1\n",
    "pour savoir combien d'url on a récupéré on peut appeler une fonction standard dans python\n",
    "len(uneliste) qui a partir d'une liste en parametres donne le nombre d'élements présents dans la liste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nb_liens1 = len(liens1)\n",
    "print nb_liens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Crawl simple d'urls\n",
    "Maintenant imaginons que nous soyons un simple crawler qui collecte toutes les url \n",
    "sans rien stocker de plus que des liens mais à profondeur infinie\n",
    "\n",
    "On a besoin d'une liste d'url à traiter et d'une liste d'url déjà traitée pour s'assurer \n",
    "que le robot ne tourne pas à l'infini et qu'il ne crawl pas plusieurs fois une même url\n",
    "pour la même raison il est utile aussi de vérifier qu'une url n'est pas en double dans les deux listes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "url_de_depart='http://www.bbc.com'\n",
    "#liste d'url a traiter\n",
    "to_do = []\n",
    "#liste d'url traitées\n",
    "done = []\n",
    "\n",
    "starter = extract_links(url_de_depart)\n",
    "# on déroule les urls de la page\n",
    "for page in starter:\n",
    "    # pour chaque lien \n",
    "    for lien in extract_links(page):\n",
    "        #on l'ajoute à faire\n",
    "        to_do.append(lien)\n",
    "    #on ajoute la page dont les liens ont déjà ete extrait\n",
    "    done.append(page)\n",
    "\n",
    "#while est une instruction spéciale en python de la manière que for c'est une boucle\n",
    "#while permet d'executer des instructions tant que la condition est remplie\n",
    "#ici tant que la liste de to_do n'est pas vide\n",
    "while len(to_do) != 0:\n",
    "    #on enlève les éventuels doublons avec la fonction de python set() qui enlève les éléments en double\n",
    "    uniq_to_do = set(to_do)\n",
    "    for lien in uniq_to_do:\n",
    "        for new_link in extract_links(lien):\n",
    "            if new_link not in done:\n",
    "                to_do.append(new_link)\n",
    "        done.append(lien)\n",
    "        to_do.remove(lien)\n",
    "        \n",
    "#ici on aura donc au final une liste de url crawlées stockées dans la liste done\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
