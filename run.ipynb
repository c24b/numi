{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cours 5\n",
    "\n",
    "### Rappel de la séance précédente"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercice 1: Rappel des étapes d'extraction\n",
    "\n",
    "Normalement vous avez déjà réussi à faire ce travail la semaine dernière,\n",
    "il s'agit simplement ici de compléter la fonction extract_chanson\n",
    "en ajoutant les méthodes pour extraire le titre, l'auteur, le texte et les tags d'une page de chanson\n",
    "à partir de BeautifulSoup.\n",
    "Vous pouvez télécharger directement le script [en cliquant ici](./rappel_script0.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#/usr/bin/python\n",
    "# coding:utf-8\n",
    "\n",
    "#ici les modules\n",
    "import requests\n",
    "import csv\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "BeautifulSoup(\"html.parser\")\n",
    "\n",
    "#ici les fonctions\n",
    "def download(url):\n",
    "    '''fonction qui télécharge une page a partir d'une url et retourne le html sous forme de texte'''\n",
    "    reponse = requests.get(url)\n",
    "    if reponse.status_code == 200:\n",
    "        return reponse.text\n",
    "\n",
    "def write_csv(filename, line):\n",
    "    '''fonction qui écrit des données sous forme de listes dans un fichier []'''\n",
    "    with open(filename, 'a') as csvfile: \n",
    "        spamwriter = csv.writer(csvfile, delimiter=';')\n",
    "        spamwriter.writerow(line)\n",
    "    return\n",
    "\n",
    "def extract_chanson(html):\n",
    "    '''ici la fonction qui permet d'extraire \n",
    "    le texte de la chanson \n",
    "    le titre\n",
    "    les tags \n",
    "    l'auteur\n",
    "    en utilisant les fonctions de Beautiful Soup\n",
    "    '''\n",
    "    soup = BeautifulSoup(html)\n",
    "    texte = \"\"\n",
    "    titre = \"\"\n",
    "    tags = \"\"\n",
    "    auteur = \"\"\n",
    "    #on utilise encode pour permettre d'écrire les accents\n",
    "    return [titre.encode(\"utf-8\"), texte.encode(\"utf-8\"), tags.encode(\"utf-8\"), auteur.encode(\"utf-8\")] \n",
    "    \n",
    "###################################################################\"\n",
    "### c'est ici que l'on execute notre programme\n",
    "### en appelant les fonctions qu'on a définit plus haut\n",
    "#**********************************#\n",
    "#Script3:\n",
    "#Tester l'extracteur de chanson de genius\n",
    "#**********************************#\n",
    "print \"Exemple3: Extraire une chanson\"\n",
    "url_d_example = \"http://genius.com/Ab-soul-terrorist-threats-lyrics\"\n",
    "html = download(url_d_example)\n",
    "chanson = extract_chanson(chanson)\n",
    "print chanson\n",
    "write_csv(\"chansons.csv\",chanson)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#script3.py\n",
    "#!/usr/bin/python \n",
    "# coding:utf-8\n",
    "\n",
    "#ici les modules\n",
    "import requests\n",
    "import csv\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "BeautifulSoup(\"html.parser\")\n",
    "\n",
    "#ici les fonctions\n",
    "def download(url):\n",
    "    '''fonction qui télécharge une page a partir d'une url et retourne le html sous forme de texte'''\n",
    "    reponse = requests.get(url)\n",
    "    if reponse.status_code == 200:\n",
    "        return reponse.text\n",
    "\n",
    "def write_csv(filename, line):\n",
    "    '''fonction qui écrit des données sous forme de listes dans un fichier []'''\n",
    "    with open(filename, 'a') as csvfile: \n",
    "        spamwriter = csv.writer(csvfile, delimiter=';')\n",
    "        spamwriter.writerow(line)\n",
    "    return\n",
    "\n",
    "def extract_links(url):\n",
    "    '''fonction qui extrait simplement les liens de la page'''\n",
    "    print url\n",
    "    html = download(url)\n",
    "    liens = []\n",
    "    soup = BeautifulSoup(html)\n",
    "    tag_link_list = soup.find_all(\"a\")\n",
    "    for element in tag_link_list:\n",
    "        lien = element.get(\"href\")\n",
    "        liens.append(lien)\n",
    "    return liens\n",
    "\n",
    "\n",
    "def extract_chanson(html):\n",
    "    '''ici la fonction qui permet d'extraire \n",
    "    le texte de la chanson \n",
    "    le titre\n",
    "    les tags \n",
    "    l'auteur\n",
    "    en utilisant les fonctions de Beautiful Soup\n",
    "    '''\n",
    "    soup = BeautifulSoup(html)    \n",
    "    texte = \"\"\n",
    "    titre = \"\"\n",
    "    tags = \"\"\n",
    "    auteur = \"\"\n",
    "    return [titre.encode(\"utf-8\"), texte.encode(\"utf-8\"), tags.encode(\"utf-8\"), auteur.encode(\"utf-8\")]\n",
    "###ici on appelle \n",
    "print \"Exemple3: La chanson de mon choix:\"\n",
    "html = download(\"http://genius.com/Ab-soul-terrorist-threats-lyrics\")\n",
    "line = extract_chanson(html)\n",
    "print line\n",
    "write_csv(\"test3.csv\", line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#### Plus de détails sur le fonctionnement de l'extraction\n",
    "\n",
    "Pour avoir le détail du fonctionnement de BeautifulSoup, \n",
    "je vous donne deux exemples détaillés\n",
    "des méthodes find et find_all et de comment récupérer des données internes à la balise \n",
    "(par exemple href ou scr):\n",
    "    \n",
    "* [exemple1: script1.py](../script1.py)\n",
    "* [exemple2: script2.py](../script2.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Introduction aux techniques de crawl en Python\n",
    "\n",
    "#### Principes du crawl\n",
    "\n",
    "Le crawl est une technique qui consiste à naviguer sur un ou plusieurs sites pour en récupérer les informations. \n",
    "\n",
    "C'est la technique qu'emploient les moteurs de recherche pour créer une base de données qui contient: 1) les références au site ; 2)les mots-clés correspondant à la recherche.\n",
    "\n",
    "La base du crawl est de \n",
    "* stocker les liens présents dans une page,\n",
    "* les télécharger au fur et à mesure\n",
    "* stocker les informations intéressantes.\n",
    "\n",
    "Le crawl permet donc de reconstituer une base de données en collectant les informations html et en les formatant pour les insérer dans une base.\n",
    "\n",
    "### Les différents types de crawl\n",
    "Il existe autant de crawlers que de besoins spécifiques. Parfois et même souvent une même extraction peut procéder de différentes manières et donc avoir différentes implémentations.\n",
    "\n",
    "Pour la collecte de données, on développe des crawlers spécifiques:  \n",
    "\n",
    "* qui suivent le parcours identifié sur le site\n",
    "* ont une profondeur fixée\n",
    "* se concentrent uniquement sur le site en question\n",
    "* extraient les informations préalablement identifiées\n",
    "* représentent souvent les données sous forme tabulaire. \n",
    "\n",
    "Par opposition, un crawler web développé par les moteurs de recherche, et qu'on appelle ici *Spider*:\n",
    "\n",
    "* suit toujours le même parcours \n",
    "* ne se concentre sur aucun site en particuler\n",
    "* n'extrait que les informations qui sont communes à tous les sites web\n",
    "* représente souvent les données sous forme de réseau ou de graphes.\n",
    "\n",
    "#### Le Spider web\n",
    "##### Algorithme et implémentation\n",
    "L'algorithme d'un spider simplifié fonctionne de la manière suivante:\n",
    "\n",
    "A partir d'une url de départ:\n",
    "        \n",
    "        * télécharger la page\n",
    "        * parser le contenu\n",
    "        * extraire toutes les urls\n",
    "        \n",
    "Pour chaque url de la liste obtenue :\n",
    "    \n",
    "    * refaire le travail\n",
    "    * et ainsi de suite jusqu'à ce qu'il n'y ait plus d'url à traiter\n",
    "\n",
    "Si on simule le fonctionnement d'un spider on écrirait un pseudo-code comme celui-ci :\n",
    "(Le pseudo-code est une manière de définir les instructions simplifiées)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tocrawl = []\n",
    "def crawl(url):\n",
    "    html = download(url)\n",
    "    page = parse(html)\n",
    "    urls = extract_links(page)\n",
    "    tocrawl.append(urls)\n",
    "    return tocrawl\n",
    "\n",
    "starter_url = \"www.example.com\"\n",
    "tocrawl = crawl(starter_url)\n",
    "while len(tocrawl) != 0:\n",
    "    for url in tocrawl:\n",
    "        crawl(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Evidemment c'est un tout petit peu plus compliqué que cela...\n",
    "Quelques exemples à la fin de ce cours\n",
    "\n",
    "* Fonctionnement du Spider de Google\n",
    "* Fonctionnement de Hyphe\n",
    "* Fonctionnement de Crawtext\n",
    "* Reprendre le fonctionnement des crawlers en ligne: détailler leur algorithme\n",
    "\n",
    "##### Représentation des données\n",
    "\n",
    "Les données du spider permettent de constituer des réseaux et des graphes en fonctions des informations collectées.\n",
    "Les réseaux constitués les plus simple constistent en graphes de citations ou co-citations. Une URL source étant reliée à plusieurs URLs par le fait qu'elle les mentionne sur sa page, on peut cartographier les sites qui se citent les uns les autres.\n",
    "\n",
    "\n",
    "####   Le crawler de site\n",
    "##### Algorithme et implémentation\n",
    "\n",
    "L'algorithme d'un crawler de site simplifié fonctionne de la manière suivante:\n",
    "Une fois le parcours sur le site et les informations identifiées, le robot aggrège les données \n",
    "cibles qu'il stocke dans une base de données ou un fichier final à plat.\n",
    "La parcours varie en fonction de l'information auquel on souhaite accéder et la manière dont on veut interroger les données.\n",
    "\n",
    "* Un exemple d'implémentation de crawl pour le site \n",
    "\n",
    "https://www.republique-numerique.fr/consultations/projet-de-loi-numerique/consultation/consultation\n",
    "\n",
    "L'objectif étant de récupérer l'ensemble des arguments pour chaque article de loi\n",
    "* Lister les URLs de tous les participants\n",
    "    Pour chaque participant:\n",
    "    * extraire les arguments qui lui appartiennent\n",
    "\n",
    "##### Représentation des données\n",
    "Le crawl produit une base de données d'arguments reliés à un article de loi.\n",
    "Dans le cas d'un crawl sur un site particulier, il est capital de définir avant le crawl les données à extraire et les différentes étapes pour appliquer l'extraction à plusieurs pages\n",
    "\n",
    "### TP: construction d'un crawler de site web\n",
    "\n",
    "##### Exercice 0: Extraire les liens d'une page web\n",
    "\n",
    "Vous pouvez télécharger le [script complet d'exemple par ici](./script0.py)\n",
    "ou copier coller dans votre éditeur le code à la suite.\n",
    "Il s'agit d'un script très simple dont il faut bien lire les instructions.\n",
    "\n",
    "La liste de toutes les URLs de cette page de départ est stockée dans liens1.\n",
    "Pour savoir combien d'URLs on a récupéré on peut appeler une fonction standard dans python: \n",
    "len(ma_liste) qui a partir d'une liste fournie en paramètres (ma_liste) nous donne le nombre d'éléments présents dans la liste\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#script0.py\n",
    "#!/usr/bin/python \n",
    "# coding:utf-8\n",
    "\n",
    "#ici les modules\n",
    "import requests\n",
    "import csv\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "BeautifulSoup(\"html.parser\")\n",
    "\n",
    "#ici les fonctions\n",
    "def download(url):\n",
    "    '''fonction qui télécharge une page a partir d'une url et retourne le html sous forme de texte'''\n",
    "    reponse = requests.get(url)\n",
    "    if reponse.status_code == 200:\n",
    "        return reponse.text\n",
    "\n",
    "def write_csv(filename, line):\n",
    "    '''fonction qui écrit des données sous forme de listes dans un fichier []'''\n",
    "    with open(filename, 'a') as csvfile: \n",
    "        spamwriter = csv.writer(csvfile, delimiter=';')\n",
    "        spamwriter.writerow(line)\n",
    "    return\n",
    "\n",
    "def extract_links(html):\n",
    "    '''fonction qui extrait simplement les liens de la page'''\n",
    "\n",
    "    liens = []\n",
    "    soup = BeautifulSoup(html)\n",
    "    tag_link_list = soup.find_all(\"a\")\n",
    "    for element in tag_link_list:\n",
    "        lien = element.get(\"href\")\n",
    "        liens.append(lien)\n",
    "    return liens\n",
    "\n",
    "###################################################################\"\n",
    "### c'est ici que l'on execute notre programme\n",
    "### en appelant les fonctions qu'on a définit plus haut\n",
    "#**********************************#\n",
    "#Exemple 0:\n",
    "#Extraire simplement les urls de la page\n",
    "#**********************************#\n",
    "print \"Exemple0: Extraire simplement les urls de la page\"\n",
    "\n",
    "url_de_depart='http://www.bbc.com/news/science_and_environment'\n",
    "html = download(url_de_depart)\n",
    "liens0 = extract_links(html)\n",
    "print \"Il y a %i urls sur la page %s\" %(len(liens0), url_de_depart)\n",
    "write_csv(\"test0.csv\",liens0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###### Exercice 1: Choississez une page\n",
    "reprenez le script0.py et enregistrez le sous script1.py\n",
    "\n",
    "Changez maintenant la partie où l'on fait tourner le code\n",
    "en changeant url_de_depart par l'URL de votre choix\n",
    "en copiant-collant l'exemple ci dessous on en téléchargeant le [code par ici](./script1.py)\n",
    "puis en le modifiant un peu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/python \n",
    "# coding:utf-8\n",
    "\n",
    "#ici les modules\n",
    "import requests\n",
    "import csv\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "BeautifulSoup(\"html.parser\")\n",
    "\n",
    "#ici les fonctions\n",
    "def download(url):\n",
    "    '''fonction qui télécharge une page a partir d'une url et retourne le html sous forme de texte'''\n",
    "    reponse = requests.get(url)\n",
    "    if reponse.status_code == 200:\n",
    "        return reponse.text\n",
    "\n",
    "def write_csv(filename, line):\n",
    "    '''fonction qui écrit des données sous forme de listes dans un fichier []'''\n",
    "    with open(filename, 'a') as csvfile: \n",
    "        spamwriter = csv.writer(csvfile, delimiter=';')\n",
    "        spamwriter.writerow(line)\n",
    "    return\n",
    "\n",
    "def extract_links(html):\n",
    "    '''fonction qui extrait simplement les liens de la page'''\n",
    "    \n",
    "    liens = []\n",
    "    soup = BeautifulSoup(html)\n",
    "    tag_link_list = soup.find_all(\"a\")\n",
    "    for element in tag_link_list:\n",
    "        lien = element.get(\"href\")\n",
    "        liens.append(lien)\n",
    "    return liens\n",
    "\n",
    "###################################################################\"\n",
    "### c'est ici que l'on execute notre programme\n",
    "### en appelant les fonctions qu'on a définit plus haut\n",
    "#**********************************#\n",
    "#Exemple 1:\n",
    "#Extraire simplement les urls de la page de mon choix\n",
    "#**********************************#\n",
    "print \"Exemple1: Extraire simplement les urls de la page de mon choix\"\n",
    "#a vous d'ajouter l'url de votre choix\n",
    "url_de_depart= \"\"\n",
    "html = download(url_de_depart)\n",
    "liens0 = extract_links(html)\n",
    "print \"Il y a %i urls sur la page %s\" %(len(liens0), url_de_depart)\n",
    "write_csv(\"test1.csv\",liens0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "Bon jusque là c'était facile, \n",
    "on vérifie juste que vous suivez!\n",
    "\n",
    "### Crawler de site: implémentation\n",
    "Maintenant que vous avez un extracteur pour une chanson il est très facile de coder un extracteur pour toutes les chansons.\n",
    "Il faut avant tout définir le parcours du crawl.\n",
    "\n",
    "Vous allez fermer vos ordinateurs pour un moment et je vous montrer l'exemple. Vous allez détailler les étapes à la main en suivant mon exemple mais seulement APRES que j'ai fait le travail sur ce site.\n",
    "\n",
    "Je cherche à extraire tous les arguments du site https://www.republique-numerique.fr/consultations/projet-de-loi-numerique/consultation/consultation pour chaque article\n",
    "\n",
    "Etant donné qu'il est très difficile de récupérer les articles et les arguments\n",
    "j'ai choisi d'attaquer par le profil des participants. Ceci constitue mon choix de départ.\n",
    "Et voici mon détail des étapes :\n",
    "\n",
    "1. J'ai défini une fonction qui à partir d'une page de profil, extrait les informations suivantes :\n",
    "[auteur, date, argument, lien_article]\n",
    "\n",
    "2. J'ai défini une fonction qui à partir de la page des participants, extrait les url d'un profil\n",
    "\n",
    "3. J'ai défini une fonction qui crée la liste de chaque page où sont listés les participants\n",
    "et génère donc une liste de page à extraire\n",
    "\n",
    "4. J'ai maintenant les éléments pour présenter mon algorithme en pseudo-code:\n",
    "\n",
    "\n",
    "    liste_pages_profiles = lister_pages_participants()\n",
    "    for page in liste_pages_profiles:\n",
    "        liste_profil = extract_url_profile(page)\n",
    "        for profile_url in liste_profil:\n",
    "            for arguments in extract_profile(profile_url):\n",
    "                csv_writer(\"arguments.csv\", arguments)\n",
    "\n",
    "Maintenant le détail de l'implémentation sans les véritables valeurs de BeautifulSoup (pour aller plus vite)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_profile(url):\n",
    "        infos = []\n",
    "        html = download(url)\n",
    "        soup = BeautifulSoup(html)\n",
    "        auteur = soup.find(\"div\", {\"class\":auteur})\n",
    "        auteur_url = url\n",
    "        arguments = soup.findall(\"div\", {\"class\": \"arguments\"})\n",
    "        for argument in arguments:\n",
    "            date = argument.find(\"div\", {\"class\":\"date\"})\n",
    "            article = argument.find(\"a\")\n",
    "            lien_article = article.get(\"href\")\n",
    "            text = argument.find(\"p\").text\n",
    "            infos.append([auteur, date, texte, lien_article])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Je suis donc déjà capable d'extraire une liste d'arguments pour un utilisateur\n",
    "en prenant exemple sur le profil suivant: https://www.republique-numerique.fr/profile/jeannevarasco"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "url = \"https://www.republique-numerique.fr/profile/jeannevarasco\"\n",
    "arguments = extract_profile(url)\n",
    "for a in arguments:\n",
    "    write_csv(\"arguments.csv\", a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mais comment récupérer toutes les pages de profil?\n",
    "\n",
    "Tous les participants sont listés à une même adresse, de la page 1 à la page 1334\n",
    "\n",
    "Je vais donc définir une fonction qui extrait l'URL du profil du participant depuis une page d'exemple\n",
    "https://www.republique-numerique.fr/projects/projet-de-loi-numerique/participants/2\n",
    "\n",
    "et qui me donne les URLs de profil dont je souhaite extraire les informations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_participants_url(url):\n",
    "    participants = []\n",
    "    html = download(url)\n",
    "    soup = BeautifulSoup(html)\n",
    "    for p in soup.find_all(\"div\"):\n",
    "        url = \"\"\n",
    "        participants.append(url)\n",
    "    return participants\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "J'ai récupéré maintenant pour une page d'exemple les 16 URLs de profils\n",
    "\n",
    "Je peux donc déjà extraire de ces 16 urls chaque profil."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "url = \"https://www.republique-numerique.fr/projects/projet-de-loi-numerique/participants/2\"\n",
    "list_participants = extract_participants_url(url)\n",
    "for profile in list_participants:\n",
    "    arguments = extract_profile(profile)\n",
    "    for a in arguments:\n",
    "        write_csv(\"arguments.csv\", a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il me faut donc maintenant tourner virtuellement les pages du site.\n",
    "\n",
    "Rien de plus simple : les pages sont numérotées!\n",
    "\n",
    "Je vais donc générer une liste d'URLs de pages structurées comme suit:\n",
    "url = \"https://www.republique-numerique.fr/projects/projet-de-loi-numerique/participants/\"+\"1\"\n",
    "etc... \n",
    "\n",
    "jusqu'à 1334\n",
    "dans une petite fonction qui s'appelle liste_pages_profiles()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def liste_pages_profiles():\n",
    "    pages = []\n",
    "    url = \"https://www.republique-numerique.fr/projects/projet-de-loi-numerique/participants/\"\n",
    "    #pour générer un numer de 1 à 1334 compris\n",
    "    for page_nb in range(1, 1335):\n",
    "        page_url = url+str(page_nb)\n",
    "        pages.append(url)\n",
    "    return pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "A vous de jouer avec genius....\n",
    "\n",
    "Lister le parcours et les extractions à chaque étape\n",
    "Puis on le codera ensemble...\n",
    "C'est parti!!!!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
